2019 12.02

NLP bert모델 tuning 결과

10만개 트레이닝, 2만개 테스트 결과

mcc = 0.9997182879217366
tp = 19240
tn = 4351
fp = 0
fn = 2
eval_loss = 0.0007250501846863052

총 19242 개의 종속항 중 실제로 종속으로 구분한 갯수 : 19240개, loss = 2
총 4351 개의 독립항 중 실제로 독립으로 구분한 갯수 : 4351개, loss = 0

evaluate 시 loss는 0.0007250501846863052로 0.07%에 불과함.

** 테스트는 49 data_set에서 학습 데이터 (1항 부터 약 100000항까지) 테스트 데이터 (200000항 부터 220000항까지) 진행
